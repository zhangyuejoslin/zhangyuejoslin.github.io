<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yue Zhang</title>
    <meta name="author" content="Yue Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <!-- Global Styles -->
    <style>
        /* Ensure images are responsive */
        .publication-img {
            max-width: 80%;
            height: auto;
            display: block;
            margin-bottom: 15px
        }

        /* Spacing between publications */
        .publication {
            padding-bottom: 100px;
            border-bottom: 1px solid #ddd;
            margin-bottom: 20px;
        }

        /* Responsive Design */
        @media screen and (max-width: 768px) {
            .publication-img {
                width: 80%;
                max-width: 400px;
            }
        }
    </style>
  </head>

  <body>
      <table style="width:100%; max-width:800px; border:10px; border-spacing:0px; border-collapse:separate; margin:auto;">
        <tbody>
          <tr>
            <td>
              <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin:auto;">
                <tbody>
                  <tr>
                    <td style="padding:2.5%; width:63%; vertical-align:middle">
                      <p class="name" style="text-align: center;">Yue Zhang</p>
                      <p>Hello! I am a postdoctoral research associate in the <a href="https://murgelab.cs.unc.edu/">MURGe-Lab</a> led by <a href="https://www.cs.unc.edu/~mbansal/">Prof. Mohit Bansal</a>, at the  University of North Carolina, Chapel Hill. I earned my Ph.D. at Michigan State University, where I was advised by <a href="https://www.cse.msu.edu/~kordjams/">Prof. Parisa Kordjamshidi</a>. I was a visiting scholar at Virginia Tech, collaborating with <a href="https://wilburone.github.io/index.html">Prof. Lifu Huang</a>. Prior to Ph.D. study, I obtained my masterâ€™s degree from Peking University. My <b>research interests</b> include multimodal learning, embodied agents, and spatial reasoning.</p>
                      <p style="text-align:center">
                        <a href="mailto:yuezhan@cs.unc.edu">Email</a> &nbsp;/&nbsp;
                        <a href="https://drive.google.com/file/d/1WM4fdTYuEKZEVnlugPHpj7IZ7yzfgUti/view?usp=sharing">CV</a> &nbsp;/&nbsp;
                        <a href="https://scholar.google.com/citations?user=moNpjqMAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                        <a href="https://github.com/zhangyuejoslin">Github</a> &nbsp;/&nbsp;
                        <a href="https://www.linkedin.com/in/zhangjoslin/">LinkedIn</a>
                      </p>
                    </td>
                    <td style="padding:4.5%; width:70%; max-width:70%;">
                      <img style="width:70%; max-width:70%; object-fit:cover; border-radius: 5%;" alt="profile photo" src="images/person.jpg">
                    </td>
                  </tr>
                </tbody>
              </table>

              <!-- News Section -->
              <h2>News</h2>
              <table style="width:100%; margin:auto;">
                <tbody>
                  <tr>
                    <td style="padding:10px; width:100%; vertical-align:middle">
                      <ul>
                        <li>[2025.11] Please check NEW preprints of <a href="https://arxiv.org/abs/2511.14086">DEER3D </a> and <a href="https://arxiv.org/abs/2511.17450">SketchVerify</a>!</li>
                       <li>[2025.8] Two papers got accepted by EMNLP 2025!</li>
                        <li>[2025.6] Please check NEW preprints of  <a href="https://www.arxiv.org/abs/2506.17113">MEXA</a> and  <a href="https://arxiv.org/abs/2505.21876">EPiC</a>.</li>
                        <li>[2025.4] Offically Joined MURGe-Lab!</li>
                        <li>[2025.2] One paper got accepted by CVPR 2025!</li>
                      </ul>
                    </td>
                  </tr>
                </tbody>
              </table>

              <!-- Publications Section -->
              <h2>Selected Publications</h2>
              <table style="width:100%; margin:auto;">
                <tbody>
                  <!-- Publication 1 -->

                  <tr class="publication">
                     <td width="40%" valign="middle">
                      <img class="publication-img" src="images/deer3d.png">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://arxiv.org/abs/2511.14086">
                        <span class="papertitle">Error-Driven Scene Editing for 3D Grounding in Large Language Models</span>
                      </a>
                      <br>
                      <strong>Yue Zhang</strong>,
                      <a href="https://zunwang1.github.io/">Zun Wang</a>,
                      <a href="https://hl-hanlin.github.io/">Han Lin</a>, 
                      <a href="https://jialuli-luka.github.io/">Jialu Li</a>, 
                      <a href="https://jedyang.com/">Jianing Yang</a>, 
                      <a href="https://yonatanbitton.github.io/">Yonatan Bitton</a>, 
                      <a href="https://sites.google.com/site/idanszpektor"> Idan Szpektor</a>, 
                      <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a> 
                      <br>
                      <em>Preprint</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2511.14086">ArXiv</a>/
                      <a href="https://github.com/zhangyuejoslin/Deer-3D">Code</a> 
                    </td>
                  </tr>
                  
                  <tr class="publication">
                     <td width="40%" valign="middle">
                      <img class="publication-img" src="images/verifier_method_website.gif">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://sketchverify.github.io/">
                        <span class="papertitle">Planning with Sketch-Guided Verification for Physics-Aware Video Generation</span>
                      </a>
                      <br>
                      <a href="https://h6kplus.github.io/owenhuang.github.io/">Yidong Huang</a>, 
                      <a href="https://zunwang1.github.io/">Zun Wang</a>,
                      <a href="https://hl-hanlin.github.io/">Han Lin</a>, 
                      <a href="https://dkkim93.github.io/">Dong-Ki Kim</a>, 
                      <a href="https://www.linkedin.com/in/shayegan/">Shayegan Omidshafiei</a>, 
                      <a href="https://jaehong31.github.io/">Jaehong Yoon</a>, 
                      <strong>Yue Zhang</strong>,
                      <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a> 
                      <br>
                      <em>Preprint</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2511.17450">ArXiv</a>/
                       <a href="sketchverify.github.io">Code</a> 
                    </td>
                  </tr>

                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/medforget.png"/>
                    </td>
                      <td width="60%" valign="middle">
                      <a href="https://arxiv.org/abs/2512.09867">
                        <span class="papertitle">MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI</span>
                      </a>
                      <br>
                      <a href="#">Fengli Wu*</a>,
                      <a href="https://vaidehi99.github.io/">Vaidehi Patil*</a>,
                      <a href="https://jaehong31.github.io/">Jaehong Yoon</a>,
                      <strong>Yue Zhang*</strong>,
                      <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a> 
                      <br>
                      <em>Preprint</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2512.09867">ArXiv</a>/
                      <a href="https://github.com/fengli-wu/MedForget">Code</a> 
                    </td>
                  </tr>
                  
                 
                  <tr class="publication">
                     <td width="40%" valign="middle">
                      <img class="publication-img" src="images/skill-vln.png">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://arxiv.org/pdf/2508.07642">
                        <span class="papertitle">Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</span>
                      </a>
                      <br>
                     <a href="https://www.egr.msu.edu/~matiany3/">Tianyi Ma</a>, 
                      <strong>Yue Zhang</strong>,
                      <a href="https://homes.esat.kuleuven.be/~zwang/">Zehao Wang</a>, 
                      <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
                      <br>
                      <em>Preprint</em>, 2025
                      <br>
                      <a href="hhttps://arxiv.org/pdf/2508.07642">ArXiv</a>/
                    </td>
                  </tr>
                  
                   <tr class="publication">
                    <td width="40%" valign="middle">
                      <video class="publication-img" width="320" height="240" controls autoplay muted loop playsinline>
                        <source src="images/epic.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                  </td>
                    <td width="60%" valign="middle">
                      <a href="https://zunwang1.github.io/Epic">
                        <span class="papertitle">EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance</span>
                      </a>
                      <br>
                      <a href="https://zunwang1.github.io/">Zun Wang</a>,
                      <a href="https://j-min.io/">Jaemin Cho</a>, 
                      <a href="https://jialuli-luka.github.io/">Jialu Li</a>, 
                      <a href="https://hl-hanlin.github.io/">Han Lin</a>, 
                      <a href="https://jaehong31.github.io/">Jaehong Yoon</a>, 
                      <strong>Yue Zhang</strong>,
                      <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a> 
                      <br>
                      <em>Preprint</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2505.21876">ArXiv</a>/
                      <a href="https://github.com/wz0919/EPiC">Code</a> 
                    </td>
                  </tr>

                   <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/dart.png"/>
                    </td>
                      <td width="60%" valign="middle">
                      <a href="https://arxiv.org/abs/2512.07132">
                        <span class="papertitle">DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning</span>
                      </a>
                      <br>
                      <a href="#">Nithin Sivakumaran</a>,
                      <a href="https://dinobby.github.io/">Justin Chih-Yao Chen</a>,
                      <a href="https://meetdavidwan.github.io/">David Wan</a>, 
                      <strong>Yue Zhang*</strong>,
                      <a href="https://jaehong31.github.io/">Jaehong Yoon</a>,
                      <a href="https://esteng.github.io/">Elias Stengel-Eskin</a>,
                      <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a> 
                      <br>
                      <em>EACL</em>, 2026
                      <br>
                      <a href="https://arxiv.org/abs/2512.07132">ArXiv</a>/
                      <a href="https://github.com/nsivaku/dart">Code</a> 
                    </td>
                  </tr>
                  

                  

                    <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/mexa.png">
                    </td>
                      <td width="60%" valign="middle">
                      <a href="https://www.arxiv.org/abs/2506.17113">
                        <span class="papertitle">MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation</span>
                      </a>
                      <br>
                      <a href="https://yui010206.github.io/">Shoubin Yu*</a>,
                      <strong>Yue Zhang*</strong>,
                      <a href="https://ziyangw2000.github.io/">Ziyang Wang</a>,
                      <a href="https://jaehong31.github.io/">Jaehong Yoon</a>, 
                      <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a> 
                      <br>
                      <em>EMNLP Findings</em>, 2025
                      <br>
                      <a href="https://www.arxiv.org/abs/2506.17113">ArXiv</a>/
                      <a href="https://github.com/Yui010206/MEXA">Code</a> 
                    </td>
                  </tr>

                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/analogical.png">
                    </td>
                      <td width="60%" valign="middle">
                      <a href="">
                        <span class="papertitle">Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs</span>
                      </a>
                      <br>
                      <strong>Yue Zhang</strong>,
                      <a href="https://www.egr.msu.edu/~matiany3/">Tianyi Ma</a>, 
                      <a href="https://zunwang1.github.io/">Zun Wang</a>, 
                      <a href="https://yanyuanqiao.github.io/">Yanyuan Qiao</a>, 
                       <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
                      <br>
                      <em>EMNLP</em>, 2025
                      <br>
                      <a href="https://www.arxiv.org/pdf/2509.25139">ArXiv</a>/
                      <a href="https://github.com/zhangyuejoslin/VLN-Analogical-Reasoning">Code</a> 
                    </td>
                  </tr>
                  
                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/M2F2_Det.png">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://arxiv.org/pdf/2503.20188">
                        <span class="papertitle">Rethinking Vision Language Model in Face Forensic: Multi-modal Interpretable Forged Face Detector</span>
                      </a>
                      <br>
                      <a href="https://chelsea234.github.io/website/">Xiao Guo</a>,
                      <a href="https://scholar.google.com/citations?user=qO93EgIAAAAJ&hl=en">Xiufeng Song</a>,
                      <strong>Yue Zhang</strong>,
                      <a href="https://jhc.sjtu.edu.cn/~xiaohongliu/">Xiaohong Liu</a>, 
                      <a href="https://cvlab.cse.msu.edu/">Xiaoming Liu</a>
                      <br>
                      <em>CVPR (<span style="color:red; font-weight:bold;">Oral</span>)</em>, 2025
                      <br>
                      <a href="https://arxiv.org/pdf/2503.20188">ArXiv</a>/
                      <a href="https://github.com/CHELSEA234/M2F2_Det">Code</a> 
                    </td>
                  </tr>

                  <!-- Publication 2 -->

                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/spartun3d.jpg">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://arxiv.org/pdf/2410.03878">
                        <span class="papertitle">SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models</span>
                      </a>
                      <br>
                      <strong>Yue Zhang</strong>, 
                      <a href="https://scholar.google.com/citations?user=Qcshi8UAAAAJ&hl=en">Zhiyang Xu</a>,
                      <a href="https://yingshen-ys.github.io/">Ying Shen</a>, 
                      <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>,
                      <a href="https://wilburone.github.io/">Lifu Huang</a>
                      <br>
                      <em>ICLR</em>, 2025
                      <br>
                      <a href="https://arxiv.org/pdf/2410.03878">ArXiv</a>/
                      <a href="https://github.com/zhangyuejoslin/Spartun3D">Code</a>
                    </td>
                  </tr>

                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/vln-survey.jpg">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://arxiv.org/abs/2407.07035">
                        <span class="papertitle">Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models</span>
                      </a>
                      <br>
                      <strong>Yue Zhang*</strong>, 
                      <a href="https://mars-tin.github.io/">Ziqiao Ma*</a>,
                      <a href="https://jialuli-luka.github.io/">Jialu Li*</a>, 
                      <a href="https://yanyuanqiao.github.io/">Yanyuan Qiao*</a>, 
                      <a href="https://zunwang1.github.io/">Zun Wang*</a>, 
                      <a href="https://web.eecs.umich.edu/~chaijy/">Joyce Chai</a>, 
                      <a href="http://qi-wu.me/">Qi Wu</a>, 
                      <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>, 
                      <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
                      <br>
                      <em>TMLR</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2407.07035">ArXiv</a>/
                      <a href="https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models">Code</a> 
                    </td>
                  </tr>

                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/acmmm.png">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3681150">
                        <span class="papertitle">Narrowing the Gap between Vision and Action in Navigation</span>
                      </a>
                      <br>
                      <strong>Yue Zhang</strong>, 
                      <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
                      <br>
                      <em>ACM MM</em>, 2024
                      <br>
                      <a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3681150">ArXiv</a>/
                      <a href="https://github.com/HLR/Dual-Action-VLN-CE">Code</a> 
                    </td>
                  </tr>

                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/common-sense.png">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://arxiv.org/pdf/2402.00126">
                        <span class="papertitle">Common Sense Reasoning for Deepfake Detection</span>
                      </a>
                      <br>
                      <strong>Yue Zhang</strong>, Ben Colman, 
                      <a href="https://chelsea234.github.io/website/">Xiao Guo</a>, Ali Shahriyari, 
                      <a href="https://gauravbharaj.github.io/">Gaurav Bharaj</a>
                      <br>
                      <em>ECCV</em>, 2024
                      <br>
                      <a href="https://arxiv.org/pdf/2402.00126">ArXiv</a>/
                      <a href="https://github.com/Reality-Defender/Research-DD-VQA">Code</a> 
                    </td>
                  </tr>

                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/navhint.png">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://arxiv.org/pdf/2402.02559.pdf">
                        <span class="papertitle">NavHint: Vision and Language Navigation Agent with a Hint Generator</span>
                      </a>
                      <br>
                      <strong>Yue Zhang</strong>, 
                      <a href="https://guoquan.net/">Quan Guo</a>, 
                      <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
                      <br>
                      <em>EACL Findings</em>, 2024
                      <br>
                      <a href="https://arxiv.org/pdf/2402.02559.pdf">ArXiv</a>/
                      <a href="https://github.com/HLR/NavHint">Code</a> 
                    </td>
                  </tr>

                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/vln-trans.png">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://arxiv.org/pdf/2302.09230.pdf">
                        <span class="papertitle">VLN-Trans: Translator for the Vision and Language Navigation Agent</span>
                      </a>
                      <br>
                      <strong>Yue Zhang</strong>,  
                      <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
                      <br>
                      <em>ACL (<span style="color:red; font-weight:bold;">Oral</span>)</em>, 2023
                      <br>
                      <a href="https://arxiv.org/pdf/2302.09230.pdf">ArXiv</a>
                      <a href="https://github.com/HLR/VLN-trans">Code</a>
                    </td>
                  </tr>

                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/lovis.png">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://aclanthology.org/2022.coling-1.505.pdf">
                        <span class="papertitle">LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation</span>
                      </a>
                      <br>
                      <strong>Yue Zhang</strong>,  
                      <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
                      <br>
                      <em>COLING (<span style="color:red; font-weight:bold;">Oral</span>)</em>, 2022
                      <br>
                      <a href="https://aclanthology.org/2022.coling-1.505.pdf">ArXiv</a>/
                      <a href="https://github.com/HLR/LOViS">Code</a> 
                    </td>
                  </tr>

                   <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/srw.png">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://aclanthology.org/2022.acl-srw.24">
                        <span class="papertitle">Explicit Object Relation Alignment for Vision and Language Navigation</span>
                      </a>
                      <br>
                      <strong>Yue Zhang</strong>,  
                      <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
                      <br>
                      <em>ACL SRW</em>, 2022
                      <br>
                      <a href="https://aclanthology.org/2022.acl-srw.24">ArXiv</a>/
                      <a href="">Code</a> 
                    </td>
                  </tr>

                  <tr class="publication">
                    <td width="40%" valign="middle">
                      <img class="publication-img" src="images/splu.png">
                    </td>
                    <td width="60%" valign="middle">
                      <a href="https://aclanthology.org/2021.splurobonlp-1.5.pdf">
                        <span class="papertitle">Towards Navigation by Reasoning over Spatial Configurations</span>
                      </a>
                      <br>
                      <strong>Yue Zhang</strong>,  
                      <a href="https://guoquan.net/">Quan Guo</a>,
                      <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
                      <br>
                      <em>ACL workshop on SpLU-RoboNLP</em>, 2021
                      <br>
                      <a href="https://aclanthology.org/2021.splurobonlp-1.5.pdf">ArXiv</a>/
                      <a href="">Code</a> 
                    </td>
                  </tr>
                </tbody>
              </table>
             

              <!-- Professional Service -->
               <h2>Professional Service</h2>
              <table style="width:100%; margin:auto;">
                <tbody>
                  <tr>
                    <td style="padding:5px; width:100%; vertical-align:middle">
                      <ul>
                        <li>[2024.7] Co-organizer of <a href="https://splu-robonlp-2024.github.io/">SpLU-RoboNLP @ ACL 2024</a></li>
                        <li>[2022.11] Invited talk at Sichuan University</li>
                        <li>Reviewer for ACL, EMNLP, NAACL</li>
                      </ul>
                    </td>
                  </tr>
                </tbody>
              </table>

            </td>
          </tr>
        </tbody>
      </table>
  </body>
</html>
